{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "ENVS = ['Ant-v2', 'Hopper-v2', 'HalfCheetah-v2', 'Humanoid-v2', 'Reacher-v2', 'Walker2d-v2']\n",
    "EXPERTS_DIR = 'experts/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michaldvorak/.cache/pypoetry/virtualenvs/cs294-hw1-py3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/michaldvorak/.cache/pypoetry/virtualenvs/cs294-hw1-py3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/michaldvorak/.cache/pypoetry/virtualenvs/cs294-hw1-py3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/michaldvorak/.cache/pypoetry/virtualenvs/cs294-hw1-py3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/michaldvorak/.cache/pypoetry/virtualenvs/cs294-hw1-py3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/michaldvorak/.cache/pypoetry/virtualenvs/cs294-hw1-py3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/michaldvorak/.cache/pypoetry/virtualenvs/cs294-hw1-py3.7/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/michaldvorak/.cache/pypoetry/virtualenvs/cs294-hw1-py3.7/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/michaldvorak/.cache/pypoetry/virtualenvs/cs294-hw1-py3.7/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/michaldvorak/.cache/pypoetry/virtualenvs/cs294-hw1-py3.7/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/michaldvorak/.cache/pypoetry/virtualenvs/cs294-hw1-py3.7/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/michaldvorak/.cache/pypoetry/virtualenvs/cs294-hw1-py3.7/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tf_util\n",
    "import gym\n",
    "import warnings\n",
    "\n",
    "REFERENCE_SEED = 42\n",
    "\n",
    "def simulation(task_name, policy_fn, max_steps=None, max_episodes=1, seed=REFERENCE_SEED):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        env = gym.make(task_name)\n",
    "        with tf.Session():\n",
    "            tf_util.initialize()\n",
    "            env.seed(seed)\n",
    "            episode_ix = 0\n",
    "            step_ix = 0\n",
    "            while not max_episodes or episode_ix < max_episodes:\n",
    "                observation = env.reset()\n",
    "                done = False                \n",
    "\n",
    "                while not done:\n",
    "                    action = policy_fn(observation[None, :])\n",
    "\n",
    "                    sample = {\n",
    "                        'observation': observation,\n",
    "                        'action': action.reshape(-1)\n",
    "                    }\n",
    "\n",
    "                    observation, reward, done, _ = env.step(action)\n",
    "\n",
    "                    sample['reward'] = reward\n",
    "\n",
    "                    yield sample\n",
    "\n",
    "                    if max_steps and step_ix >= max_steps:\n",
    "                        break\n",
    "\n",
    "                    step_ix += 1\n",
    "\n",
    "                if max_steps and step_ix >= max_steps:\n",
    "                    break\n",
    "\n",
    "                episode_ix += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_policy import load_policy\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "\n",
    "DEMONSTRATION_STEPS = 100000\n",
    "DEMONSTRATIONS_DIR = 'demonstrations'\n",
    "\n",
    "\n",
    "def load_expert_policy(task_name):\n",
    "    policy_file_path = os.path.join(EXPERTS_DIR, f'{task_name}.pkl')\n",
    "    return load_policy(policy_file_path)\n",
    "\n",
    "\n",
    "def save_demonstration(demonstration_file_path, demonstration):\n",
    "    with open(demonstration_file_path, 'wb') as fp:\n",
    "        pickle.dump(demonstration, fp)\n",
    "\n",
    "        \n",
    "def load_demonstration(demonstration_file_path):\n",
    "    with open(demonstration_file_path, 'rb') as fp:\n",
    "        return pickle.load(fp)\n",
    "\n",
    "\n",
    "def get_demonstration(task_name, steps=100000, seed=42):\n",
    "    os.makedirs(DEMONSTRATIONS_DIR, exist_ok=True)\n",
    "    demonstration_file_path = os.path.join(DEMONSTRATIONS_DIR, f'{task_name}-demonstration-steps-{steps}-seed-{seed}.pkl')\n",
    "    \n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        if not os.path.exists(demonstration_file_path):\n",
    "            demonstration = simulation(task_name, load_expert_policy(task_name), max_episodes=None, max_steps=steps, seed=seed)\n",
    "            save_demonstration(demonstration_file_path, list(demonstration))\n",
    "    \n",
    "    return load_demonstration(demonstration_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def get_model(observation_size, action_size, hidden_sizes=None, divider=5):   \n",
    "    if hidden_sizes is None:\n",
    "        hidden_sizes = []\n",
    "        hidden_size = int(observation_size / divider)\n",
    "        while hidden_size > action_size*1.5:\n",
    "            hidden_sizes.append(hidden_size)\n",
    "            hidden_size = int(hidden_size / divider)\n",
    "        \n",
    "    shapes = [observation_size] + hidden_sizes + [action_size]\n",
    "    \n",
    "    layers = []\n",
    "    for ix in range(len(shapes) - 2):\n",
    "        layers.append(torch.nn.Linear(shapes[ix], shapes[ix + 1]))\n",
    "        layers.append(torch.nn.ReLU())\n",
    "        \n",
    "    layers.append(torch.nn.Linear(shapes[-2], shapes[-1]))\n",
    "    \n",
    "    return torch.nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class Simulation(Dataset):\n",
    "    def __init__(self, simulation_data):\n",
    "        self.simulation_data = simulation_data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.simulation_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'observation': self.simulation_data[idx]['observation'].astype('float32'),\n",
    "            'action': self.simulation_data[idx]['action'].astype('float32')\n",
    "        }\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        return Simulation(self.simulation_data + other.simulation_data)\n",
    "    \n",
    "    def __iadd__(self, other):\n",
    "        return self + other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def model_to_policy(model):\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    def policy_fn(observation):\n",
    "        with torch.no_grad():\n",
    "            return model(torch.tensor(observation.reshape(1, -1).astype('float32')).to(device)).cpu().numpy().reshape(1, -1)\n",
    "    \n",
    "    return policy_fn\n",
    "\n",
    "def get_eval_fn(task_name, seed=111):\n",
    "    # TODO - evaluate on multiple seeds\n",
    "    def eval_fn(model):\n",
    "        results = simulation(task_name, model_to_policy(model), seed=seed, max_episodes=1)\n",
    "        rewards = [result['reward'] for result in results]\n",
    "        return sum(rewards)\n",
    "    return eval_fn\n",
    "\n",
    "def train_model(model, dataset, epochs, batch_size=1024, shuffle=True, learning_rate=1e-3, loss_fn=None, eval_fn=None, verbose=True):\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, drop_last=True)\n",
    "    \n",
    "    if loss_fn is None:\n",
    "        loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "        \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    \n",
    "    losses = []\n",
    "    test_rewards = []\n",
    "    for epoch in range(epochs):\n",
    "        epoch_losses = []\n",
    "        for batch in loader:\n",
    "            action_pred = model(batch['observation'].to(device))\n",
    "            loss = loss_fn(action_pred, batch['action'].to(device))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()        \n",
    "            optimizer.step()\n",
    "            epoch_losses.append(loss.item())\n",
    "        \n",
    "        epoch_loss = sum(epoch_losses)/len(epoch_losses) if len(epoch_losses) > 0 else 0\n",
    "        losses.append(epoch_loss)\n",
    "        \n",
    "        if eval_fn is not None:\n",
    "            test_reward = eval_fn(model)\n",
    "            test_rewards.append(test_reward)\n",
    "            if verbose:\n",
    "                print(f'Epoch {epoch + 1}: loss = {round(epoch_loss, 2)}, test_reward = {round(test_reward, 2)}')\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(f'Epoch {epoch + 1}: loss = {round(epoch_loss, 2)}')\n",
    "        \n",
    "    return losses, test_rewards\n",
    "    \n",
    "\n",
    "def train_behavioral_cloning(demonstration, epochs, batch_size=1024, learning_rate=1e-3, eval_fn=None, verbose=True):\n",
    "    observation_size = demonstration[0]['observation'].shape[0]\n",
    "    action_size = demonstration[0]['action'].shape[0]\n",
    "    dataset = Simulation(demonstration)\n",
    "    model = get_model(observation_size, action_size)\n",
    "    \n",
    "    lossed, test_rewards = train_model(model, dataset, epochs, batch_size=batch_size, learning_rate=learning_rate, eval_fn=eval_fn, verbose=verbose)\n",
    "    return model, lossed, test_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0816 14:06:36.326490 140180404811200 deprecation_wrapper.py:119] From /home/michaldvorak/projects/courses/cs294/cs294-hws/hw1/load_policy.py:57: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Ant-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0816 14:06:37.212540 140180404811200 deprecation.py:323] From /home/michaldvorak/projects/courses/cs294/cs294-hws/hw1/tf_util.py:118: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Please use tf.global_variables instead.\n",
      "W0816 14:06:37.213543 140180404811200 deprecation_wrapper.py:119] From /home/michaldvorak/projects/courses/cs294/cs294-hws/hw1/tf_util.py:97: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0816 14:06:37.214074 140180404811200 deprecation.py:323] From /home/michaldvorak/.cache/pypoetry/virtualenvs/cs294-hw1-py3.7/lib/python3.7/site-packages/tensorflow/python/util/tf_should_use.py:193: initialize_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.variables_initializer` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: loss = 381.93, test_reward = 981.31\n",
      "Epoch 2: loss = 159.68, test_reward = 1272.92\n",
      "Epoch 3: loss = 117.01, test_reward = 124.19\n",
      "Epoch 4: loss = 96.12, test_reward = 1678.24\n",
      "Epoch 5: loss = 83.5, test_reward = 1928.89\n",
      "Epoch 6: loss = 73.5, test_reward = 2082.23\n",
      "Epoch 7: loss = 64.66, test_reward = 1849.33\n",
      "Epoch 8: loss = 56.92, test_reward = 3510.02\n",
      "Epoch 9: loss = 50.51, test_reward = 3782.06\n",
      "Epoch 10: loss = 45.31, test_reward = 4443.77\n",
      "Epoch 11: loss = 41.16, test_reward = 4276.66\n",
      "Epoch 12: loss = 37.87, test_reward = 4217.24\n",
      "Epoch 13: loss = 35.15, test_reward = 4307.15\n",
      "Epoch 14: loss = 32.92, test_reward = 4668.71\n",
      "Epoch 15: loss = 31.31, test_reward = 4626.72\n",
      "Trained with test reward 4632.286540747935 (expert has 4916.8448828056).\n",
      "Training Hopper-v2\n",
      "Epoch 1: loss = 12514.63, test_reward = 176.38\n",
      "Epoch 2: loss = 6587.25, test_reward = 57.58\n",
      "Epoch 3: loss = 4309.12, test_reward = 38.8\n",
      "Epoch 4: loss = 3483.54, test_reward = 40.26\n",
      "Epoch 5: loss = 3151.11, test_reward = 40.14\n",
      "Epoch 6: loss = 2962.02, test_reward = 43.79\n",
      "Epoch 7: loss = 2816.64, test_reward = 47.2\n",
      "Epoch 8: loss = 2689.52, test_reward = 55.87\n",
      "Epoch 9: loss = 2573.04, test_reward = 64.72\n",
      "Epoch 10: loss = 2466.42, test_reward = 68.12\n",
      "Epoch 11: loss = 2368.97, test_reward = 67.85\n",
      "Epoch 12: loss = 2275.36, test_reward = 71.5\n",
      "Epoch 13: loss = 2189.82, test_reward = 73.45\n",
      "Epoch 14: loss = 2105.88, test_reward = 75.28\n",
      "Epoch 15: loss = 2027.7, test_reward = 75.44\n",
      "Trained with test reward 77.0850192683703 (expert has 3778.6012554655663).\n",
      "Training HalfCheetah-v2\n",
      "Epoch 1: loss = 32819.1, test_reward = -3076.16\n",
      "Epoch 2: loss = 7234.7, test_reward = -1515.58\n",
      "Epoch 3: loss = 3435.23, test_reward = -556.79\n",
      "Epoch 4: loss = 2143.06, test_reward = 207.0\n",
      "Epoch 5: loss = 1549.12, test_reward = 484.72\n",
      "Epoch 6: loss = 1244.39, test_reward = 744.94\n",
      "Epoch 7: loss = 1073.96, test_reward = 911.02\n",
      "Epoch 8: loss = 968.5, test_reward = 719.9\n",
      "Epoch 9: loss = 895.22, test_reward = 334.05\n",
      "Epoch 10: loss = 839.09, test_reward = 382.36\n",
      "Epoch 11: loss = 793.71, test_reward = 474.09\n",
      "Epoch 12: loss = 755.7, test_reward = 545.69\n",
      "Epoch 13: loss = 724.02, test_reward = 498.56\n",
      "Epoch 14: loss = 697.44, test_reward = 469.23\n",
      "Epoch 15: loss = 675.22, test_reward = 350.21\n",
      "Trained with test reward 359.3476307035297 (expert has 4053.149521204849).\n",
      "Training Humanoid-v2\n",
      "Epoch 1: loss = 126502.51, test_reward = 134.2\n",
      "Epoch 2: loss = 7384.57, test_reward = 157.09\n",
      "Epoch 3: loss = 4913.69, test_reward = 239.0\n",
      "Epoch 4: loss = 3831.06, test_reward = 221.49\n",
      "Epoch 5: loss = 3204.94, test_reward = 207.13\n",
      "Epoch 6: loss = 2792.6, test_reward = 216.9\n",
      "Epoch 7: loss = 2519.9, test_reward = 270.17\n",
      "Epoch 8: loss = 2303.25, test_reward = 256.99\n",
      "Epoch 9: loss = 2134.16, test_reward = 282.99\n",
      "Epoch 10: loss = 2021.18, test_reward = 340.27\n",
      "Epoch 11: loss = 1911.17, test_reward = 543.59\n",
      "Epoch 12: loss = 1809.83, test_reward = 400.53\n",
      "Epoch 13: loss = 1730.01, test_reward = 304.59\n",
      "Epoch 14: loss = 1664.7, test_reward = 344.45\n",
      "Epoch 15: loss = 1604.95, test_reward = 302.49\n",
      "Trained with test reward 705.4481961837264 (expert has 10402.960541284101).\n",
      "Training Reacher-v2\n",
      "Epoch 1: loss = 158.98, test_reward = -10.37\n",
      "Epoch 2: loss = 41.07, test_reward = -8.93\n",
      "Epoch 3: loss = 20.44, test_reward = -10.62\n",
      "Epoch 4: loss = 15.43, test_reward = -11.5\n",
      "Epoch 5: loss = 13.61, test_reward = -12.03\n",
      "Epoch 6: loss = 12.77, test_reward = -12.03\n",
      "Epoch 7: loss = 12.22, test_reward = -12.1\n",
      "Epoch 8: loss = 11.83, test_reward = -12.1\n",
      "Epoch 9: loss = 11.55, test_reward = -12.12\n",
      "Epoch 10: loss = 11.34, test_reward = -12.11\n",
      "Epoch 11: loss = 11.18, test_reward = -12.14\n",
      "Epoch 12: loss = 11.05, test_reward = -12.14\n",
      "Epoch 13: loss = 10.94, test_reward = -12.15\n",
      "Epoch 14: loss = 10.84, test_reward = -12.16\n",
      "Epoch 15: loss = 10.76, test_reward = -12.17\n",
      "Trained with test reward -9.167178859782389 (expert has -3.463437423328769).\n",
      "Training Walker2d-v2\n",
      "Epoch 1: loss = 30538.18, test_reward = -4.64\n",
      "Epoch 2: loss = 9788.24, test_reward = 249.84\n",
      "Epoch 3: loss = 4860.89, test_reward = 17.59\n",
      "Epoch 4: loss = 3333.16, test_reward = 11.97\n",
      "Epoch 5: loss = 2744.93, test_reward = 15.23\n",
      "Epoch 6: loss = 2448.52, test_reward = 243.97\n",
      "Epoch 7: loss = 2262.72, test_reward = 310.47\n",
      "Epoch 8: loss = 2126.87, test_reward = 793.09\n",
      "Epoch 9: loss = 2018.74, test_reward = 465.69\n",
      "Epoch 10: loss = 1929.91, test_reward = 507.67\n",
      "Epoch 11: loss = 1854.04, test_reward = 481.4\n",
      "Epoch 12: loss = 1788.0, test_reward = 415.79\n",
      "Epoch 13: loss = 1731.02, test_reward = 371.3\n",
      "Epoch 14: loss = 1681.38, test_reward = 613.43\n",
      "Epoch 15: loss = 1636.52, test_reward = 356.67\n",
      "Trained with test reward 370.1715220767571 (expert has 5573.229261637097).\n"
     ]
    }
   ],
   "source": [
    "models_bc = {}\n",
    "for env in ENVS:\n",
    "    print(f'Training {env}')\n",
    "    model, losses, test_rewards = train_behavioral_cloning(get_demonstration(env), epochs=15, eval_fn=get_eval_fn(env), verbose=True)\n",
    "    test_results = simulation(env, model_to_policy(model), seed=REFERENCE_SEED)\n",
    "                              \n",
    "    models_bc[env] = {\n",
    "        'model': model,\n",
    "        'losses': losses,\n",
    "        'test_reward': sum([result['reward'] for result in test_results])\n",
    "    }\n",
    "    \n",
    "    expert_results = simulation(env, load_expert_policy(env), max_episodes=1, seed=REFERENCE_SEED)\n",
    "    expert_reward = sum([result['reward'] for result in expert_results])\n",
    "    \n",
    "    print(f'Trained with test reward {models_bc[env][\"test_reward\"]} (expert has {expert_reward}).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def train_dagger(task_name, episodes=20):\n",
    "    # Set up an expert policy for the task\n",
    "    expert_policy = load_expert_policy(task_name)\n",
    "    \n",
    "    # Train a model using behavioral cloning first\n",
    "    expert_results = list(simulation(task_name,  expert_policy, max_episodes=1, seed=random.randint(0,1000)))\n",
    "    observation_size = expert_results[0]['observation'].shape[0]\n",
    "    action_size = expert_results[0]['action'].shape[0]\n",
    "    dataset = Simulation(expert_results)\n",
    "    model = get_model(observation_size, action_size)\n",
    "    dataset = Simulation(expert_results)\n",
    "    batch_size = min(512, 2 ** int(np.log2(len(dataset))))\n",
    "    train_model(model, dataset, batch_size=batch_size, epochs=15, eval_fn=get_eval_fn(task_name), verbose=False)\n",
    "    \n",
    "    # Compute reference reward for the task\n",
    "    reference_results = simulation(env, load_expert_policy(env), max_episodes=1, seed=REFERENCE_SEED)\n",
    "    reference_reward = sum([result['reward'] for result in expert_results])\n",
    "    \n",
    "    # For several episodes do\n",
    "    for episode in range(episodes):\n",
    "        # Simulate the models behaviour in the gym\n",
    "        model_results = simulation(task_name,  model_to_policy(model), max_episodes=1, seed=random.randint(0,1000))\n",
    "        \n",
    "        # For the observations encountered in the simulation, find ground truths using an expert policy\n",
    "        expert_labeling = [\n",
    "            {\n",
    "                'observation': result['observation'],\n",
    "                'action': expert_policy(result['observation'].reshape(1, -1)).reshape(-1)\n",
    "            }\n",
    "            for result in model_results\n",
    "        ]\n",
    "        \n",
    "        # Aggregate datasets\n",
    "        dataset += Simulation(expert_labeling)\n",
    "        \n",
    "        # Train model using the aggregated dataset\n",
    "        batch_size = min(512, 2 ** int(np.log2(len(dataset))))\n",
    "        total_steps = 100000\n",
    "        epochs = int(total_steps / len(dataset)) + 1\n",
    "        train_model(model, dataset, batch_size=batch_size, epochs=epochs, eval_fn=get_eval_fn(task_name), verbose=False)\n",
    "        \n",
    "        test_results = simulation(task_name, model_to_policy(model), seed=REFERENCE_SEED)\n",
    "    \n",
    "        print(f'Current test reward {sum([result[\"reward\"] for result in test_results])} (expert has {expert_reward}).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Ant-v2\n",
      "Current test reward 1024.1180524975766 (expert has 5573.229261637097).\n",
      "Current test reward 875.364491336192 (expert has 5573.229261637097).\n",
      "Current test reward 1117.3069519722367 (expert has 5573.229261637097).\n",
      "Current test reward 3606.7900773617393 (expert has 5573.229261637097).\n",
      "Current test reward 4224.554977761744 (expert has 5573.229261637097).\n",
      "Current test reward 4509.974596031048 (expert has 5573.229261637097).\n",
      "Current test reward 4462.199129061074 (expert has 5573.229261637097).\n",
      "Current test reward 4413.0848901239315 (expert has 5573.229261637097).\n",
      "Current test reward 4607.194981554653 (expert has 5573.229261637097).\n",
      "Current test reward 4165.257273123266 (expert has 5573.229261637097).\n",
      "Current test reward 4721.180581602416 (expert has 5573.229261637097).\n",
      "Current test reward 4756.178484701388 (expert has 5573.229261637097).\n",
      "Current test reward 4588.653749438652 (expert has 5573.229261637097).\n",
      "Current test reward 4620.150873353496 (expert has 5573.229261637097).\n",
      "Current test reward 4648.515801851051 (expert has 5573.229261637097).\n",
      "Current test reward 4807.99800132913 (expert has 5573.229261637097).\n",
      "Current test reward 4585.644613882401 (expert has 5573.229261637097).\n",
      "Current test reward 4774.208667635632 (expert has 5573.229261637097).\n",
      "Current test reward 4779.819001596816 (expert has 5573.229261637097).\n",
      "Current test reward 4949.725541481522 (expert has 5573.229261637097).\n",
      "Training Hopper-v2\n",
      "Current test reward 31.993103281974417 (expert has 5573.229261637097).\n",
      "Current test reward 78.91320681988759 (expert has 5573.229261637097).\n",
      "Current test reward 63.81358665955156 (expert has 5573.229261637097).\n",
      "Current test reward 69.25444242496764 (expert has 5573.229261637097).\n",
      "Current test reward 80.30944742314581 (expert has 5573.229261637097).\n",
      "Current test reward 90.84909259993215 (expert has 5573.229261637097).\n",
      "Current test reward 97.59467069412523 (expert has 5573.229261637097).\n",
      "Current test reward 105.7653147534498 (expert has 5573.229261637097).\n",
      "Current test reward 111.94082011311777 (expert has 5573.229261637097).\n",
      "Current test reward 124.73516408752674 (expert has 5573.229261637097).\n",
      "Current test reward 157.13478809062568 (expert has 5573.229261637097).\n",
      "Current test reward 360.72703105761514 (expert has 5573.229261637097).\n",
      "Current test reward 367.34105582830273 (expert has 5573.229261637097).\n",
      "Current test reward 366.9113844146001 (expert has 5573.229261637097).\n",
      "Current test reward 373.15874405002495 (expert has 5573.229261637097).\n",
      "Current test reward 489.25403240593664 (expert has 5573.229261637097).\n",
      "Current test reward 521.6643607919781 (expert has 5573.229261637097).\n",
      "Current test reward 547.4874183760055 (expert has 5573.229261637097).\n",
      "Current test reward 689.3215218668265 (expert has 5573.229261637097).\n",
      "Current test reward 537.7744940851325 (expert has 5573.229261637097).\n",
      "Training HalfCheetah-v2\n",
      "Current test reward 31.882698383428565 (expert has 5573.229261637097).\n",
      "Current test reward -27.419026717509162 (expert has 5573.229261637097).\n",
      "Current test reward -40.96446839846093 (expert has 5573.229261637097).\n",
      "Current test reward -95.13186658864029 (expert has 5573.229261637097).\n",
      "Current test reward -53.757972098501305 (expert has 5573.229261637097).\n",
      "Current test reward -75.24770624595004 (expert has 5573.229261637097).\n",
      "Current test reward -79.056875287529 (expert has 5573.229261637097).\n",
      "Current test reward -45.06864812354817 (expert has 5573.229261637097).\n",
      "Current test reward -49.87827833646894 (expert has 5573.229261637097).\n",
      "Current test reward -55.04441630088887 (expert has 5573.229261637097).\n",
      "Current test reward -65.01041796138529 (expert has 5573.229261637097).\n",
      "Current test reward -75.32058084830243 (expert has 5573.229261637097).\n",
      "Current test reward -94.98445319968131 (expert has 5573.229261637097).\n",
      "Current test reward -114.1412239075534 (expert has 5573.229261637097).\n",
      "Current test reward -134.4308780684834 (expert has 5573.229261637097).\n",
      "Current test reward -133.96538976595647 (expert has 5573.229261637097).\n",
      "Current test reward -129.44685212445674 (expert has 5573.229261637097).\n",
      "Current test reward -126.229080053281 (expert has 5573.229261637097).\n",
      "Current test reward -122.053756146456 (expert has 5573.229261637097).\n",
      "Current test reward -119.47571268513445 (expert has 5573.229261637097).\n",
      "Training Humanoid-v2\n",
      "Current test reward 270.94411291829556 (expert has 5573.229261637097).\n",
      "Current test reward 245.96877219985424 (expert has 5573.229261637097).\n",
      "Current test reward 488.9774097997796 (expert has 5573.229261637097).\n",
      "Current test reward 317.29873723196204 (expert has 5573.229261637097).\n",
      "Current test reward 438.79176248961016 (expert has 5573.229261637097).\n",
      "Current test reward 1100.1765201890078 (expert has 5573.229261637097).\n",
      "Current test reward 402.04924909455144 (expert has 5573.229261637097).\n",
      "Current test reward 594.6304540070203 (expert has 5573.229261637097).\n",
      "Current test reward 536.4961997159352 (expert has 5573.229261637097).\n",
      "Current test reward 511.12282160839624 (expert has 5573.229261637097).\n",
      "Current test reward 365.7399044496592 (expert has 5573.229261637097).\n",
      "Current test reward 358.822572778507 (expert has 5573.229261637097).\n",
      "Current test reward 430.2221147714212 (expert has 5573.229261637097).\n",
      "Current test reward 394.99955666374683 (expert has 5573.229261637097).\n",
      "Current test reward 492.93011355645496 (expert has 5573.229261637097).\n",
      "Current test reward 1238.0367815186473 (expert has 5573.229261637097).\n",
      "Current test reward 1262.0973684770565 (expert has 5573.229261637097).\n",
      "Current test reward 1039.9859898974057 (expert has 5573.229261637097).\n",
      "Current test reward 4315.95091160583 (expert has 5573.229261637097).\n",
      "Current test reward 464.2961142675812 (expert has 5573.229261637097).\n",
      "Training Reacher-v2\n",
      "Current test reward -9.733339521329187 (expert has 5573.229261637097).\n",
      "Current test reward -10.68220870048831 (expert has 5573.229261637097).\n",
      "Current test reward -8.050126265720099 (expert has 5573.229261637097).\n",
      "Current test reward -7.897458146522855 (expert has 5573.229261637097).\n",
      "Current test reward -7.894748015154057 (expert has 5573.229261637097).\n",
      "Current test reward -5.9318560458859535 (expert has 5573.229261637097).\n",
      "Current test reward -5.925459430574485 (expert has 5573.229261637097).\n",
      "Current test reward -6.988332566282336 (expert has 5573.229261637097).\n",
      "Current test reward -7.239058633978166 (expert has 5573.229261637097).\n",
      "Current test reward -7.622483569707329 (expert has 5573.229261637097).\n",
      "Current test reward -7.6375312200058625 (expert has 5573.229261637097).\n",
      "Current test reward -8.099204116932373 (expert has 5573.229261637097).\n",
      "Current test reward -7.064807718969933 (expert has 5573.229261637097).\n",
      "Current test reward -6.737541327558535 (expert has 5573.229261637097).\n",
      "Current test reward -6.970225117117155 (expert has 5573.229261637097).\n",
      "Current test reward -7.01448756278302 (expert has 5573.229261637097).\n",
      "Current test reward -7.359399626728262 (expert has 5573.229261637097).\n",
      "Current test reward -7.391920953610535 (expert has 5573.229261637097).\n",
      "Current test reward -7.406881511166381 (expert has 5573.229261637097).\n",
      "Current test reward -7.5771086466287025 (expert has 5573.229261637097).\n",
      "Training Walker2d-v2\n",
      "Current test reward 1.4334030461866472 (expert has 5573.229261637097).\n",
      "Current test reward 2.801299331901077 (expert has 5573.229261637097).\n",
      "Current test reward 271.9984610377903 (expert has 5573.229261637097).\n",
      "Current test reward 1012.5682816638971 (expert has 5573.229261637097).\n",
      "Current test reward 4.446185729914224 (expert has 5573.229261637097).\n",
      "Current test reward 4.768462583721496 (expert has 5573.229261637097).\n",
      "Current test reward 320.41714694469846 (expert has 5573.229261637097).\n",
      "Current test reward 314.0795570006695 (expert has 5573.229261637097).\n",
      "Current test reward 328.9621427374301 (expert has 5573.229261637097).\n",
      "Current test reward 723.3277649832971 (expert has 5573.229261637097).\n",
      "Current test reward 946.2767272490006 (expert has 5573.229261637097).\n",
      "Current test reward 842.3443093160224 (expert has 5573.229261637097).\n",
      "Current test reward 818.286448814153 (expert has 5573.229261637097).\n",
      "Current test reward 838.7110315447203 (expert has 5573.229261637097).\n",
      "Current test reward 809.3238528629786 (expert has 5573.229261637097).\n",
      "Current test reward 958.0316112205388 (expert has 5573.229261637097).\n",
      "Current test reward 661.498037944049 (expert has 5573.229261637097).\n",
      "Current test reward 711.8915958691392 (expert has 5573.229261637097).\n",
      "Current test reward 674.8381748795085 (expert has 5573.229261637097).\n",
      "Current test reward 671.6850270803942 (expert has 5573.229261637097).\n"
     ]
    }
   ],
   "source": [
    "for env in ENVS:\n",
    "    print(f'Training DAgger {env}')\n",
    "    train_dagger(env)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
